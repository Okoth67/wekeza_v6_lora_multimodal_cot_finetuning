{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03e43b5-9b7f-4b99-aa5e-178d5078f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up\n",
    "!pip install transformers peft accelerate datasets bitsandbytes --quiet\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import torch, json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a962351f-24d6-401b-8bff-4f48b869f938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 90 clean examples.\n",
      "Skipped 1 bad lines.\n",
      "{'instruction': 'How do I start investing in a money market fund in Kenya?', 'input': '', 'output': 'To invest in a money market fund (MMF) in Kenya, choose a licensed fund manager such as CIC, Britam, or NCBA. Fill out their application form (usually online), submit a copy of your ID and KRA PIN, then deposit the minimum investment amount via bank or mobile money. Your money begins earning daily interest, and you can monitor performance via monthly statements or mobile apps.'}\n"
     ]
    }
   ],
   "source": [
    "#loading the dataset\n",
    "import json\n",
    "\n",
    "jsonl_path = r\"C:\\Users\\bbollo\\Downloads\\kenyan_finance_selfinstruct_v6_instructzero.jsonl\"\n",
    "\n",
    "base_data = []\n",
    "bad_lines = []\n",
    "\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        try:\n",
    "            clean_line = line.strip()\n",
    "            if clean_line:  # skip blank lines\n",
    "                base_data.append(json.loads(clean_line))\n",
    "        except json.JSONDecodeError:\n",
    "            bad_lines.append((i, line))\n",
    "\n",
    "print(f\"Loaded {len(base_data)} clean examples.\")\n",
    "print(f\"Skipped {len(bad_lines)} bad lines.\")\n",
    "if base_data:\n",
    "    print(base_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb42b897-bc22-4e90-b1e8-bb87d6c9e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining prompt templates for the 3 modalities\n",
    "def generate_modal_rationales(prompt, model, tokenizer):\n",
    "    nl_prompt = f\"{prompt}\\nExplain in plain language:\"\n",
    "    code_prompt = f\"{prompt}\\nWrite a Python function to reason this out:\"\n",
    "    table_prompt = f\"{prompt}\\nMake a truth-table style comparison:\"\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300)\n",
    "    return {\n",
    "        \"nl\": pipe(nl_prompt)[0]['generated_text'],\n",
    "        \"code\": pipe(code_prompt)[0]['generated_text'],\n",
    "        \"table\": pipe(table_prompt)[0]['generated_text'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d27a649d-a1b2-476f-a298-57369e6f50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering based on modality\n",
    "def filter_rationales(raw):\n",
    "    def clean(text):\n",
    "        return text.strip() if len(text.split()) > 10 else None\n",
    "    return {\n",
    "        \"nl\": clean(raw[\"nl\"]),\n",
    "        \"code\": clean(raw[\"code\"]) if \"def\" in raw[\"code\"] else None,\n",
    "        \"table\": clean(raw[\"table\"]) if \"|\" in raw[\"table\"] else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb65935e-3e83-40d3-85b2-d0f64d173b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "C:\\Users\\bbollo\\AppData\\Local\\anaconda3\\envs\\wekeza\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:1803: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"./distilgpt2-wekeza-finetuned_v5_cot_lora\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d939c1-fd14-46c1-a974-e06ba21df052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Alpaca-style formatting with modal reasoning tags\n",
    "test_data = base_data[:3]\n",
    "\n",
    "multi_modal_data = []\n",
    "\n",
    "for item in test_data:\n",
    "    rationale = generate_modal_rationales(item[\"instruction\"], model, tokenizer)\n",
    "    filtered = filter_rationales(rationale)\n",
    "\n",
    "    for mode, output in filtered.items():\n",
    "        if output and isinstance(output, str) and output.strip():\n",
    "            multi_modal_data.append({\n",
    "                \"instruction\": item[\"instruction\"].strip() + f\" [{mode} reasoning]\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": output.strip()\n",
    "            })\n",
    "\n",
    "with open(\"wekeza_multimodal_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(multi_modal_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(multi_modal_data)} test examples to 'wekeza_multimodal_test.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce82519-37c5-4845-8ce1-d2563b90e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetuning with LoRA\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import torch\n",
    "model_path = \"./distilgpt2-wekeza-finetuned_v5_cot_lora\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float32)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\"],  # Specific to GPT2/DistilGPT2\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "dataset = load_dataset(\"json\", data_files=\"wekeza_multimodal.json\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b691bcf-fed9-444e-aa6e-af086e863a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilgpt2-wekeza-finetuned_v6_lora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=torch.cuda.is_available(),  # Only enable fp16 if on GPU\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
